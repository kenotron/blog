{"data":{"ghostAuthor":{"slug":"ken","name":"Ken Chau","bio":null,"cover_image":null,"profile_image":"//www.gravatar.com/avatar/45fc02e659e008e7b53c0b8ee8dd71a1?s=250&d=mm&r=x","location":null,"website":null,"twitter":"@kenotron","facebook":null},"allGhostPost":{"edges":[{"node":{"id":"Ghost__Post__5c9a41371046a4000156bcf3","title":"Localize React without Bloating the Bundle","slug":"localize-react-apps-the-performant-way","featured":false,"feature_image":"http://localhost:2368/content/images/2019/03/roman-kraft-136249-unsplash-1.jpg","excerpt":"There are so many possibilities when you want to localize your application with\nyour React application. I believe that localization is difficult because it\nrequires excellence in several pieces of a stack at the same time. You have to\nhave a working pipeline that can take string resources that would get\ntranslated. Then, you have to have a way to load these strings onto your page or\napplication. Finally, you have to have a way to take these strings and inject\nthem into the components inside your","custom_excerpt":null,"created_at_pretty":"26 March, 2019","published_at_pretty":"26 March, 2019","updated_at_pretty":"27 March, 2019","created_at":"2019-03-26T15:11:51.000+00:00","published_at":"2019-03-26T17:55:16.000+00:00","updated_at":"2019-03-27T19:36:37.000+00:00","meta_title":null,"meta_description":null,"og_description":null,"og_image":null,"og_title":null,"twitter_description":null,"twitter_image":null,"twitter_title":null,"authors":[{"name":"Ken Chau","slug":"ken","bio":null,"profile_image":"//www.gravatar.com/avatar/45fc02e659e008e7b53c0b8ee8dd71a1?s=250&d=mm&r=x","twitter":"@kenotron","facebook":null,"website":null}],"primary_author":{"name":"Ken Chau","slug":"ken","bio":null,"profile_image":"//www.gravatar.com/avatar/45fc02e659e008e7b53c0b8ee8dd71a1?s=250&d=mm&r=x","twitter":"@kenotron","facebook":null,"website":null},"primary_tag":null,"tags":[],"plaintext":"There are so many possibilities when you want to localize your application with\nyour React application. I believe that localization is difficult because it\nrequires excellence in several pieces of a stack at the same time. You have to\nhave a working pipeline that can take string resources that would get\ntranslated. Then, you have to have a way to load these strings onto your page or\napplication. Finally, you have to have a way to take these strings and inject\nthem into the components inside your application. As you can see, you can choose\njust about any tech to help you accomplish these goals. I am documenting a\nparticular set of stack that I believe helps you achieve this in the most\nperformant way with tools that you're already probably using.\n\nTranslation Pipeline\nAt work [https://github.com/officedev/office-ui-fabric-react], we have a\ntranslation-as-a-service API that we rely on to refresh localized strings for us\nevery night. There's a growing team that has built a simple Azure DevOps\n[https://devops.azure.com]  task we add as a step in one of our pipelines which\nruns nightly. \n\nNot everyone is as fortunate that has something they can use from their own\ncompany. Given that, I'll suggest a pattern here as the first step. Do a search\nfor \"localization as a service\" and look for a vendor that can help add a step\nin your CI pipeline of choice. Set up a nightly job to refresh your\napplication's localized strings as JSON like this:\n\n{\n  \"HELLO_NAME\": \"Hello {name}!\",\n  \"CLICK_ME\": \"Click me\"\n}\n\n\nRendering Localized Strings\nReact is a large ecosystem. So then the paradox of choice is real when finding\nsupplemental libraries for React. Conventional wisdom is to find the most\npopular packages from npmjs.org. So given this, I first looked at react-intl  to\nhelp me inject localized strings into my application. The issue here is that \nreact-intl  uses higher order components all over the place. One of the explicit\ngoals (as I heard it from ReactConf 2018) of React hooks is to do away with\ndepth of the component tree caused by the higher order components. Higher order\ncomponent, or HOC, is a neat idea until the consumer needed to access the ref to\nthe original wrapped component. When all your components that use localized\nstrings are wrapped in HOCs, your application start to look like a sideways\nmountain. (aside: go look at your component tree in React DevTool to see if\nyou're suffering from HOC-itis)\n\nEnter react-intl-universal [https://github.com/alibaba/react-intl-universal].\nThe Alibaba Group created this library to get around the HOC issues of \nreact-intl. On top of this, there are times where strings are needed from\noutside of the component's render()  method. It takes 2 steps to place strings\ninside your components. \n\nFirst you have to initialize the locale data. Note that the data can be\npreloaded from a server or can be retrieved at runtime. The choice is yours. For\nthe most optimal case, we definitely would have the server preload strings right\nin the app as it is being loaded.\n\nLet's pretend that ./locales/en-US.json  has the same content as the example\nabove.\n\n// locale data\nconst locales = {\n  \"en-US\": require('./locales/en-US.json'),\n  \"zh-CN\": require('./locales/zh-CN.json'),\n};\n\n\nThen, we initialize the react-intl-universal  library inside a \ncomponentDidMount()  call. And we'll use the localized string inside the \nrender()  method with the .get()  function: \n\nimport intl from 'react-intl-universal'; \n\nclass App extends Component {\n  state = { isLoading: false }\n  componentDidMount() {\n    this.loadLocales();\n  }\n\n  async loadLocales() {\n    await intl.init({\n      currentLocale: 'en-US',\n      locales,\n    });\n    this.setState({ isLoading: true });\n  }\n\n  render() {\n    return (\n      !this.state.isLoading &&\n      <div>\n        {intl.get('HELLO_NAME', {name: 'world'})}\n      </div>\n    );\n  }\n\n}\n\n\nNote that the init()  call returns a Promise. This means that we can use the\nasync / await syntax to write our string load code. Once this is added, we look\nat the way we retrieve the strings by key. For that, we use the get(). Get takes\nin two parameters: the key and some object. Sometimes the strings have slots\nthat can be replaced by the object values.\n\nLoading Localized Strings\nThis is where it gets interesting. So far, we've assumed that we had the locale\ndata all upfront. This means that all the localized strings would had been\nloaded inside a bundle or onto the page somehow. Loading all the language\nstrings in one go can only be feasible if the app barely contain any text. If\nwe're using Webpack, we should take advantage of a feature that I recently came\nto know. We all have seen the dynamic import()  syntax:\n\nconst SomeModule = import('some-module');\n\n\nBut, have you seen what Webpack can do with something like this?\n\nconst getLocale = (locale) => import(`./locales/${locale}.json`);\n\n\nBased on the .json  files it finds inside ./locales, Webpack is smart enough to\ngenerate chunks for dynamic loading! That means your main bundle will not incur\nthe weight of the entire library of localized strings. Putting all these\nconcepts together, I've created a repo to demonstrate concepts from this post:\n\nhttps://github.com/kenotron/react-intl-example\n\nI'll go over some of the points from that repo. First, I created a HOC that you\nplace at the ROOT of the application. Don't worry! It is only one HOC for the\nentire app. It is called LocaleComponent  - I'm keeping this strange little name\nuntil React.createResource()  becomes a thing maybe in the future.\n\nconst getLocale = locale => import(`./locale/${locale}.json`);\n\nclass LocaleComponent extends React.Component {\n  state = { isLoading: true };\n  \n  async loadLocales() { \n    const locales = await getLocale('en');\n    const currentLocale = 'en';\n    await intl.init({ currentLocale, locales });\n    this.setState({ isLoading: false });\n  }\n  \n  render() {\n    return !this.state.isLoading ? <>this.props.children</> : null;\n  }\n}","html":"<p>There are so many possibilities when you want to localize your application with your React application. I believe that localization is difficult because it requires excellence in several pieces of a stack at the same time. You have to have a working pipeline that can take string resources that would get translated. Then, you have to have a way to load these strings onto your page or application. Finally, you have to have a way to take these strings and inject them into the components inside your application. As you can see, you can choose just about any tech to help you accomplish these goals. I am documenting a particular set of stack that I believe helps you achieve this in the most performant way with tools that you're already probably using.</p><h2 id=\"translation-pipeline\">Translation Pipeline</h2><p>At <a href=\"https://github.com/officedev/office-ui-fabric-react\">work</a>, we have a translation-as-a-service API that we rely on to refresh localized strings for us every night. There's a growing team that has built a simple <a href=\"https://devops.azure.com\">Azure DevOps</a> task we add as a step in one of our pipelines which runs nightly. </p><p>Not everyone is as fortunate that has something they can use from their own company. Given that, I'll suggest a pattern here as the first step. Do a search for \"localization as a service\" and look for a vendor that can help add a step in your CI pipeline of choice. Set up a nightly job to refresh your application's localized strings as JSON like this:</p><!--kg-card-begin: markdown--><pre><code class=\"language-json\">{\n  &quot;HELLO_NAME&quot;: &quot;Hello {name}!&quot;,\n  &quot;CLICK_ME&quot;: &quot;Click me&quot;\n}\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"rendering-localized-strings\">Rendering Localized Strings</h2><p>React is a large ecosystem. So then the paradox of choice is real when finding supplemental libraries for React. Conventional wisdom is to find the most popular packages from npmjs.org. So given this, I first looked at <code>react-intl</code> to help me inject localized strings into my application. The issue here is that <code>react-intl</code> uses higher order components all over the place. One of the explicit goals (as I heard it from ReactConf 2018) of React hooks is to do away with depth of the component tree caused by the higher order components. Higher order component, or HOC, is a neat idea until the consumer needed to access the ref to the original wrapped component. When all your components that use localized strings are wrapped in HOCs, your application start to look like a sideways mountain. (aside: go look at your component tree in React DevTool to see if you're suffering from HOC-itis)</p><p>Enter <code><a href=\"https://github.com/alibaba/react-intl-universal\">react-intl-universal</a></code>. The Alibaba Group created this library to get around the HOC issues of <code>react-intl</code>. On top of this, there are times where strings are needed from outside of the component's <code>render()</code> method. It takes 2 steps to place strings inside your components. </p><p>First you have to initialize the locale data. Note that the data can be preloaded from a server or can be retrieved at runtime. The choice is yours. For the most optimal case, we definitely would have the server preload strings right in the app as it is being loaded.</p><p>Let's pretend that <code>./locales/en-US.json</code> has the same content as the example above.</p><!--kg-card-begin: markdown--><pre><code class=\"language-jsx\">// locale data\nconst locales = {\n  &quot;en-US&quot;: require('./locales/en-US.json'),\n  &quot;zh-CN&quot;: require('./locales/zh-CN.json'),\n};\n</code></pre>\n<!--kg-card-end: markdown--><p>Then, we initialize the <code>react-intl-universal</code> library inside a <code>componentDidMount()</code> call. And we'll use the localized string inside the <code>render()</code> method with the <code>.get()</code> function: </p><!--kg-card-begin: markdown--><pre><code class=\"language-jsx\">import intl from 'react-intl-universal'; \n\nclass App extends Component {\n  state = { isLoading: false }\n  componentDidMount() {\n    this.loadLocales();\n  }\n\n  async loadLocales() {\n    await intl.init({\n      currentLocale: 'en-US',\n      locales,\n    });\n    this.setState({ isLoading: true });\n  }\n\n  render() {\n    return (\n      !this.state.isLoading &amp;&amp;\n      &lt;div&gt;\n        {intl.get('HELLO_NAME', {name: 'world'})}\n      &lt;/div&gt;\n    );\n  }\n\n}\n</code></pre>\n<!--kg-card-end: markdown--><p>Note that the <code>init()</code> call returns a <code>Promise</code>. This means that we can use the async / await syntax to write our string load code. Once this is added, we look at the way we retrieve the strings by key. For that, we use the <code>get()</code>. Get takes in two parameters: the key and some object. Sometimes the strings have slots that can be replaced by the object values.</p><h3 id=\"loading-localized-strings\">Loading Localized Strings</h3><p>This is where it gets interesting. So far, we've assumed that we had the locale data all upfront. This means that all the localized strings would had been loaded inside a bundle or onto the page somehow. Loading all the language strings in one go can only be feasible if the app barely contain any text. If we're using Webpack, we should take advantage of a feature that I recently came to know. We all have seen the dynamic <code>import()</code> syntax:</p><!--kg-card-begin: markdown--><pre><code class=\"language-jx\">const SomeModule = import('some-module');\n</code></pre>\n<!--kg-card-end: markdown--><p>But, have you seen what Webpack can do with something like this?</p><!--kg-card-begin: markdown--><pre><code class=\"language-js\">const getLocale = (locale) =&gt; import(`./locales/${locale}.json`);\n</code></pre>\n<!--kg-card-end: markdown--><p>Based on the <code>.json</code> files it finds inside <code>./locales</code>, Webpack is smart enough to generate chunks for dynamic loading! That means your main bundle will not incur the weight of the entire library of localized strings. Putting all these concepts together, I've created a repo to demonstrate concepts from this post:</p><p><a href=\"https://github.com/kenotron/react-intl-example\">https://github.com/kenotron/react-intl-example</a></p><p>I'll go over some of the points from that repo. First, I created a HOC that you place at the ROOT of the application. Don't worry! It is only one HOC for the entire app. It is called <code>LocaleComponent</code> - I'm keeping this strange little name until <code>React.createResource()</code> becomes a thing maybe in the future.</p><!--kg-card-begin: markdown--><pre><code class=\"language-jsx\">const getLocale = locale =&gt; import(`./locale/${locale}.json`);\n\nclass LocaleComponent extends React.Component {\n  state = { isLoading: true };\n  \n  async loadLocales() { \n    const locales = await getLocale('en');\n    const currentLocale = 'en';\n    await intl.init({ currentLocale, locales });\n    this.setState({ isLoading: false });\n  }\n  \n  render() {\n    return !this.state.isLoading ? &lt;&gt;this.props.children&lt;/&gt; : null;\n  }\n}\n</code></pre>\n<!--kg-card-end: markdown-->","url":"http://localhost:2368/localize-react-apps-the-performant-way/","uuid":"86d08ddb-d015-45d6-8f74-9a5affb6b63e","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c9a41371046a4000156bcf3"}},{"node":{"id":"Ghost__Post__5c9aa8bfb6d76300017e5840","title":"Speeding Up Webpack, Typescript Incremental Builds by 7x","slug":"speeding-up-webpack-typescript-incremental-builds-by-7x","featured":false,"feature_image":"https://images.unsplash.com/photo-1534078362425-387ae9668c17?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=1080&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ","excerpt":"What if I told you your Webpack is doing too much work all this time? Webpack 4\nbrought a lot of goodies for developers but to use it at scale, the Outlook\n[http://outlook.com/]  team at Microsoft had to take a hard look at the\nincremental build numbers to find out. Here’s how we made our incremental builds\ngo from 35s to a consistent 5s.\n\nI guess it goes without saying that you MUST measure in order for you to know\nyou have made progress!\n\nLet’s name some enemies of incremental build speed:\n\n 1","custom_excerpt":null,"created_at_pretty":"26 March, 2019","published_at_pretty":"06 April, 2018","updated_at_pretty":"26 March, 2019","created_at":"2019-03-26T22:33:35.000+00:00","published_at":"2018-04-06T22:58:00.000+00:00","updated_at":"2019-03-26T22:59:55.000+00:00","meta_title":null,"meta_description":null,"og_description":null,"og_image":null,"og_title":null,"twitter_description":null,"twitter_image":null,"twitter_title":null,"authors":[{"name":"Ken Chau","slug":"ken","bio":null,"profile_image":"//www.gravatar.com/avatar/45fc02e659e008e7b53c0b8ee8dd71a1?s=250&d=mm&r=x","twitter":"@kenotron","facebook":null,"website":null}],"primary_author":{"name":"Ken Chau","slug":"ken","bio":null,"profile_image":"//www.gravatar.com/avatar/45fc02e659e008e7b53c0b8ee8dd71a1?s=250&d=mm&r=x","twitter":"@kenotron","facebook":null,"website":null},"primary_tag":null,"tags":[],"plaintext":"What if I told you your Webpack is doing too much work all this time? Webpack 4\nbrought a lot of goodies for developers but to use it at scale, the Outlook\n[http://outlook.com/]  team at Microsoft had to take a hard look at the\nincremental build numbers to find out. Here’s how we made our incremental builds\ngo from 35s to a consistent 5s.\n\nI guess it goes without saying that you MUST measure in order for you to know\nyou have made progress!\n\nLet’s name some enemies of incremental build speed:\n\n 1. stats.toJson [https://webpack.js.org/api/node/#stats-tojson-options-]()\n 2. Competing resolution logic between Webpack and its loaders (ts-loader\n    [https://github.com/TypeStrong/ts-loader])\n 3. Garbage Collection\n 4. Subtle v8 ES6 perfomance issues\n\nThe Base Line\nTo begin, there are already a few things beyond  setting the mode in \nwebpack.config.js  we already  apply so we’re not doing too much optimization\nduring incremental builds:\n\noptimization: {\n  removeAvailableModules: false,\n  removeEmptyChunks: false,\n  splitChunks: false,\n}\n\n\nAlright, so let’s establish the baseline by looking at a typical inner loop\nflame graph:\n\nAs you can see, we’re clocking in at around 40s here per incremental build. This\nis not exactly true because we lose about 5s of it due to profiling. In\nmeasuring with our internal telemetry, we noticed that our devs are hitting\naround 30–35s on avg (and sometimes over a minute at the 75th percentile)\nincremental builds.\n\nSo, as soon as you look at those colors, you would recognize three separate\nphases of the incremental build process. With this in mind, let’s tackle the\nfirst enemy.\n\nEnemy #1: stats.toJson is VERY heavy in WP4\nIf you were looking at the the CPU profile flame graph, you would notice that\nthe last phase of the process is dominated by a bunch of stats.toJson  calls.\nWhere does it come from? It’s right inside webpack-dev-server’s Server.js:\n\nconst clientStats = { errorDetails: false };\n...\ncomp.hooks.done.tap('webpack-dev-server', (stats) => {\n  this._sendStats(this.sockets, stats.toJson(clientStats));\n  this._stats = stats;\n});\n\n\nThe issue here is that Webpack 4 gave toJson a lot more information, but it also\nregressed the performance tremendously as a result. The fix is in a pull\nrequest:\n\nhttps://github.com/webpack/webpack-dev-server/pull/1362\n\nThis is the big one — it brought our incremental speeds from 30s to around 15s.\n\nUpdate: the webpack-dev-server maintainers had accepted my patch! So, go ahead\nand use webpack-dev-server@3.1.2. Personally, I have observed a slight 0.5s\nregression between 2.x release and the 3.x release, so we’re keeping the 2.x for\nnow until we can move to using webpack-serve.\n\nSince we’re waiting for the authors to merge this for v2, I’ve published a\ntemporary fork for it for the v2 branch:\n\nhttps://www.npmjs.com/package/webpack-dev-server-speedy\n\nFor the fix on v2, you’ll have to use the node API to take advantage of it like\nthis package in your build process:\n\nconst Webpack = require('webpack');\nconst WebpackDevServer = require('webpack-dev-server-speedy');\nconst webpackConfig = require('./webpack.config');\nconst compiler = Webpack(webpackConfig);\nconst devServerOptions = Object.assign({}, webpackConfig.devServer, {\n  stats: {\n    colors: true\n  }\n});\nconst server = new WebpackDevServer(compiler, devServerOptions);\nserver.listen(8080, '127.0.0.1', () => {\n  console.log('Starting server on http://localhost:8080');\n});\n\n\nEnemy #2: Competing resolution logic between Webpack and ts-loader\nIf I were to ask you to build an incremental compiler based on Typescript, you\nwould likely first reach into the Typescript API for something that it is using\nfor its watch mode. For the longest time, Typescript safe guarded this API from\nexternal modules. During this time, ts-loader was born. The author of the loader\ntracked the progress of another Typescript-centric loader called\nawesome-typescript-loader and brought back the idea of doing type checking on a\nseparate thread. This transpileOnly  flag worked remarkably well (with a rather\nglaring caveat that const enums are not supported out of the box — here’s a \nworkaround [https://github.com/kulshekhar/ts-jest#const-enum-is-not-supported] \nfrom the ts-jest repo) until the codebase reaches a certain size.\n\nIn OWA, we have nearly 9000 modules that we shove across this loader. We have\nfound that the first phase of that incremental build is linearly growing as our\nrepo grows.\n\nThings looked pretty grim until the Typescript team decided to take on this\nmammoth work of expose the watch API to external modules. Specifically, after \nthis [https://github.com/TypeStrong/ts-loader/pull/685]  was merged, ts-loader\nis super charged with the ability to limit the amount of modules to transpile at\na time per iteration!\n\nWe just add this to our webpack config module.rules:\n\n{\n  test: /\\.tsx?$/,\n  use: [\n    {\n      loader: 'ts-loader',\n      options: {\n        transpileOnly: true,\n        experimentalWatchApi: true,\n      },\n    },\n  ],\n}\n\n\nDon’t forget to the typechecker when appropriate: \nhttps://www.npmjs.com/package/fork-ts-checker-webpack-plugin  (we have a mode to\nturn type checker OFF for even faster rebuilds)\n\nThe incremental builds now only rebuilds around 30–40 modules rather than 50% of\nour modules! I also have a way to CAP the growth of the incremental builds in\nthe first phase.\n\nThis optimization cuts our 15s to around 8s.\n\nEnemy #3: Garbage Collection\nSo Garbage Collection is a great invention. But not in a tight loop. Perhaps\nthere’s a perf bug inside node or v8, but I’ve discovered that a global \nstring.replace(/…/g, ‘…..’) can cause a lot of GC  when placed inside a loop.\nWebpack 4 introduced the path info in the generated dev mode replacing module\nids with more useful path info. This is done with, you guessed it, global string\nreplace with regex. It then created a LOT of unnecessary GCs along the way. (as\nan aside, perhaps I should file a bug against either Webpack, node, or v8…)\n\nOkay, let’s turn that sucker off in webpack.config.js in the output.pathinfo:\n\noutput: {\n  pathinfo: false\n}\n\nJust ask yourself if you REALLY need that pathinfo or that build speed. For us,\nwe chose speed. This made our 8s builds to around 6s\n\nEnemy #4: Subtle v8 ES6 perfomance issues\nMost everyone would be pleased with that 6s figure, but why should we humans not\ndemand MOAR? Yes, MOAR speed!!!\n\nIn chatting with a colleague of mine, John-David Dalton\n[https://github.com/jdalton], about his project, esm\n[https://medium.com/web-on-the-edge/tomorrows-es-modules-today-c53d29ac448c], he\ntold me about node.js performance issues with ES6 data structures like Map and\nSet. Having dug into Webpack source code previously and  by looking at the\nremaining profile slowdowns (looking at the “heavy” or “bottom-up”), I noticed\nthat Webpack’s internal algorithm is dominated by calling their SortableSet\nmethods. Since SortableSet extends Set, it would follow that Webpack is actually\ngreatly affected by the speed of the Map/Set implementation of V8. Here’s the\nbug:\n\nhttps://github.com/nodejs/node/issues/19769\n\nSo, I advise everyone doing heavy Webpack development to switch to the LTS (v10+\nor stick with v8.9.4)\n\nUsing that version, the incremental build is down to 4.5s\n\nWhy? Inventing on Principle!\nFinally, I want to leave you with the best motivation on why we should reduce\nthis incremental build speeds down to almost nothing:\n\nHey! follow me on twitter @kenneth_chau [https://twitter.com/kenneth_chau]  to\nget more articles like these :)","html":"<p>What if I told you your Webpack is doing too much work all this time? Webpack 4 brought a lot of goodies for developers but to use it at scale, the <a href=\"http://outlook.com/\" rel=\"nofollow noopener noopener\">Outlook</a> team at Microsoft had to take a hard look at the incremental build numbers to find out. Here’s how we made our incremental builds go from 35s to a consistent 5s.</p><p><em>I guess it goes without saying that you MUST measure in order for you to know you have made progress!</em></p><p>Let’s name some enemies of incremental build speed:</p><ol><li><a href=\"https://webpack.js.org/api/node/#stats-tojson-options-\" rel=\"noopener nofollow\">stats.toJson</a>()</li><li>Competing resolution logic between Webpack and its loaders (<a href=\"https://github.com/TypeStrong/ts-loader\" rel=\"noopener nofollow\">ts-loader</a>)</li><li>Garbage Collection</li><li>Subtle v8 ES6 perfomance issues</li></ol><h3 id=\"the-base-line\">The Base Line</h3><p>To begin, there are already a few things <em>beyond</em> setting the mode in <code>webpack.config.js</code> we <em>already</em> apply so we’re not doing too much optimization during incremental builds:</p><!--kg-card-begin: markdown--><pre><code class=\"language-js\">optimization: {\n  removeAvailableModules: false,\n  removeEmptyChunks: false,\n  splitChunks: false,\n}\n</code></pre>\n<!--kg-card-end: markdown--><p>Alright, so let’s establish the baseline by looking at a typical inner loop flame graph:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card\"><img src=\"http://localhost:2368/content/images/2019/03/image.png\" class=\"kg-image\"></figure><!--kg-card-end: image--><p>As you can see, we’re clocking in at around 40s here per incremental build. This is not exactly true because we lose about 5s of it due to profiling. In measuring with our internal telemetry, we noticed that our devs are hitting around 30–35s on avg (and sometimes over a minute at the 75th percentile) incremental builds.</p><p>So, as soon as you look at those colors, you would recognize three separate phases of the incremental build process. With this in mind, let’s tackle the first enemy.</p><h3 id=\"enemy-1-stats-tojson-is-very-heavy-in-wp4\">Enemy #1: stats.toJson is VERY heavy in WP4</h3><p>If you were looking at the the CPU profile flame graph, you would notice that the last phase of the process is dominated by a bunch of <strong><strong>stats.toJson</strong></strong> calls. Where does it come from? It’s right inside <strong><strong>webpack-dev-server</strong></strong>’s Server.js:</p><!--kg-card-begin: markdown--><pre><code class=\"language-js\">const clientStats = { errorDetails: false };\n...\ncomp.hooks.done.tap('webpack-dev-server', (stats) =&gt; {\n  this._sendStats(this.sockets, stats.toJson(clientStats));\n  this._stats = stats;\n});\n</code></pre>\n<!--kg-card-end: markdown--><p>The issue here is that Webpack 4 gave toJson a lot more information, but it also regressed the performance tremendously as a result. The fix is in a pull request:</p><p><a href=\"https://github.com/webpack/webpack-dev-server/pull/1362\">https://github.com/webpack/webpack-dev-server/pull/1362</a></p><p><strong><strong>This is the big one — it brought our incremental speeds from 30s to around 15s.</strong></strong></p><p><strong><strong>Update</strong></strong>: the webpack-dev-server maintainers had accepted my patch! So, go ahead and use webpack-dev-server@3.1.2. Personally, I have observed a slight 0.5s regression between 2.x release and the 3.x release, so we’re keeping the 2.x for now until we can move to using webpack-serve.</p><p>Since we’re waiting for the authors to merge this for v2, I’ve published a temporary fork for it for the v2 branch:</p><p><a href=\"https://www.npmjs.com/package/webpack-dev-server-speedy\">https://www.npmjs.com/package/webpack-dev-server-speedy</a></p><p>For the fix on v2, you’ll have to use the node API to take advantage of it like this package in your build process:</p><!--kg-card-begin: markdown--><pre><code class=\"language-js\">const Webpack = require('webpack');\nconst WebpackDevServer = require('webpack-dev-server-speedy');\nconst webpackConfig = require('./webpack.config');\nconst compiler = Webpack(webpackConfig);\nconst devServerOptions = Object.assign({}, webpackConfig.devServer, {\n  stats: {\n    colors: true\n  }\n});\nconst server = new WebpackDevServer(compiler, devServerOptions);\nserver.listen(8080, '127.0.0.1', () =&gt; {\n  console.log('Starting server on http://localhost:8080');\n});\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"enemy-2-competing-resolution-logic-between-webpack-and-ts-loader\">Enemy #2: Competing resolution logic between Webpack and ts-loader</h3><p>If I were to ask you to build an incremental compiler based on Typescript, you would likely first reach into the Typescript API for something that it is using for its watch mode. For the longest time, Typescript safe guarded this API from external modules. During this time, ts-loader was born. The author of the loader tracked the progress of another Typescript-centric loader called awesome-typescript-loader and brought back the idea of doing type checking on a separate thread. This <em>transpileOnly</em> flag worked remarkably well (with a rather glaring caveat that const enums are not supported out of the box — here’s a <a href=\"https://github.com/kulshekhar/ts-jest#const-enum-is-not-supported\" rel=\"noopener nofollow\">workaround</a> from the ts-jest repo) until the codebase reaches a certain size.</p><p>In OWA, we have nearly 9000 modules that we shove across this loader. We have found that the first phase of that incremental build is linearly growing as our repo grows.</p><p>Things looked pretty grim until the Typescript team decided to take on this mammoth work of expose the watch API to external modules. Specifically, after <a href=\"https://github.com/TypeStrong/ts-loader/pull/685\" rel=\"noopener nofollow\">this</a> was merged, ts-loader is super charged with the ability to limit the amount of modules to transpile at a time per iteration!</p><p>We just add this to our webpack config <strong><strong>module.rules</strong></strong>:</p><!--kg-card-begin: markdown--><pre><code>{\n  test: /\\.tsx?$/,\n  use: [\n    {\n      loader: 'ts-loader',\n      options: {\n        transpileOnly: true,\n        experimentalWatchApi: true,\n      },\n    },\n  ],\n}\n</code></pre>\n<!--kg-card-end: markdown--><p><em>Don’t forget to the typechecker when appropriate: </em><a href=\"https://www.npmjs.com/package/fork-ts-checker-webpack-plugin\" rel=\"nofollow noopener nofollow noopener\"><em>https://www.npmjs.com/package/fork-ts-checker-webpack-plugin</em></a><em> (we have a mode to turn type checker OFF for even faster rebuilds)</em></p><p>The incremental builds now only rebuilds around 30–40 modules rather than 50% of our modules! I also have a way to CAP the growth of the incremental builds in the first phase.</p><p><strong><strong>This optimization cuts our 15s to around 8s</strong></strong>.</p><h3 id=\"enemy-3-garbage-collection\"><strong>Enemy #3: Garbage Collection</strong></h3><p>So Garbage Collection is a great invention. But not in a tight loop. Perhaps there’s a perf bug inside node or v8, but I’ve discovered that a global <strong><strong>string.replace(/…/g, ‘…..’) can cause a lot of GC</strong></strong> when placed inside a loop. Webpack 4 introduced the path info in the generated dev mode replacing module ids with more useful path info. This is done with, you guessed it, global string replace with regex. It then created a LOT of unnecessary GCs along the way. (<em>as an aside, perhaps I should file a bug against either Webpack, node, or v8…</em>)</p><p>Okay, let’s turn that sucker off in webpack.config.js in the output.pathinfo:</p><!--kg-card-begin: code--><pre><code>output: {\n  pathinfo: false\n}</code></pre><!--kg-card-end: code--><p>Just ask yourself if you REALLY need that pathinfo or that build speed. For us, we chose speed. <strong><strong>This made our 8s builds to around 6s</strong></strong></p><h3 id=\"enemy-4-subtle-v8-es6-perfomance-issues\">Enemy #4: Subtle v8 ES6 perfomance issues</h3><p>Most everyone would be pleased with that 6s figure, but why should we humans not demand MOAR? Yes, MOAR speed!!!</p><p>In chatting with a colleague of mine, <a href=\"https://github.com/jdalton\" rel=\"nofollow noopener\">John-David Dalton</a>, about his project, <a href=\"https://medium.com/web-on-the-edge/tomorrows-es-modules-today-c53d29ac448c\">esm</a>, he told me about node.js performance issues with ES6 data structures like Map and Set. Having dug into Webpack source code previously <em>and</em> by looking at the remaining profile slowdowns (looking at the “heavy” or “bottom-up”), I noticed that Webpack’s internal algorithm is dominated by calling their SortableSet methods. Since SortableSet extends Set, it would follow that Webpack is actually greatly affected by the speed of the Map/Set implementation of V8. Here’s the bug:</p><p><a href=\"https://github.com/nodejs/node/issues/19769\">https://github.com/nodejs/node/issues/19769</a></p><p>So, I advise everyone doing heavy Webpack development to switch to the LTS (v10+ or stick with v8.9.4)</p><p><strong><strong>Using that version, the incremental build is down to 4.5s</strong></strong></p><h3 id=\"why-inventing-on-principle-\">Why? Inventing on Principle!</h3><p>Finally, I want to leave you with the best motivation on why we should reduce this incremental build speeds down to almost nothing:</p><!--kg-card-begin: embed--><figure class=\"kg-card kg-embed-card\"><iframe src=\"https://player.vimeo.com/video/36579366?app_id=122963\" width=\"480\" height=\"270\" frameborder=\"0\" title=\"Bret Victor - Inventing on Principle\" allow=\"autoplay; fullscreen\" allowfullscreen></iframe></figure><!--kg-card-end: embed--><p>Hey! follow me on twitter <a href=\"https://twitter.com/kenneth_chau\">@kenneth_chau</a> to get more articles like these :)</p>","url":"http://localhost:2368/speeding-up-webpack-typescript-incremental-builds-by-7x/","uuid":"6d4ea64f-f9fa-4ce9-9133-7f5963a8a862","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c9aa8bfb6d76300017e5840"}}]}},"pageContext":{"slug":"ken","limit":12,"skip":0,"numberOfPages":1,"humanPageNumber":1,"prevPageNumber":null,"nextPageNumber":null,"previousPagePath":null,"nextPagePath":null}}