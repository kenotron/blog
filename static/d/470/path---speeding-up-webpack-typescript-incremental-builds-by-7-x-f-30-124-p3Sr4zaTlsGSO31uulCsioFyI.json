{"data":{"ghostPost":{"id":"Ghost__Post__5c9aa8bfb6d76300017e5840","title":"Speeding Up Webpack, Typescript Incremental Builds by 7x","slug":"speeding-up-webpack-typescript-incremental-builds-by-7x","featured":false,"feature_image":"https://images.unsplash.com/photo-1534078362425-387ae9668c17?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=1080&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ","excerpt":"What if I told you your Webpack is doing too much work all this time? Webpack 4\nbrought a lot of goodies for developers but to use it at scale, the Outlook\n[http://outlook.com/]  team at Microsoft had to take a hard look at the\nincremental build numbers to find out. Here’s how we made our incremental builds\ngo from 35s to a consistent 5s.\n\nI guess it goes without saying that you MUST measure in order for you to know\nyou have made progress!\n\nLet’s name some enemies of incremental build speed:\n\n 1","custom_excerpt":null,"created_at_pretty":"26 March, 2019","published_at_pretty":"06 April, 2018","updated_at_pretty":"26 March, 2019","created_at":"2019-03-26T22:33:35.000+00:00","published_at":"2018-04-06T22:58:00.000+00:00","updated_at":"2019-03-26T22:59:55.000+00:00","meta_title":null,"meta_description":null,"og_description":null,"og_image":null,"og_title":null,"twitter_description":null,"twitter_image":null,"twitter_title":null,"authors":[{"name":"Ken Chau","slug":"ken","bio":null,"profile_image":"//www.gravatar.com/avatar/45fc02e659e008e7b53c0b8ee8dd71a1?s=250&d=mm&r=x","twitter":"@kenotron","facebook":null,"website":null}],"primary_author":{"name":"Ken Chau","slug":"ken","bio":null,"profile_image":"//www.gravatar.com/avatar/45fc02e659e008e7b53c0b8ee8dd71a1?s=250&d=mm&r=x","twitter":"@kenotron","facebook":null,"website":null},"primary_tag":null,"tags":[],"plaintext":"What if I told you your Webpack is doing too much work all this time? Webpack 4\nbrought a lot of goodies for developers but to use it at scale, the Outlook\n[http://outlook.com/]  team at Microsoft had to take a hard look at the\nincremental build numbers to find out. Here’s how we made our incremental builds\ngo from 35s to a consistent 5s.\n\nI guess it goes without saying that you MUST measure in order for you to know\nyou have made progress!\n\nLet’s name some enemies of incremental build speed:\n\n 1. stats.toJson [https://webpack.js.org/api/node/#stats-tojson-options-]()\n 2. Competing resolution logic between Webpack and its loaders (ts-loader\n    [https://github.com/TypeStrong/ts-loader])\n 3. Garbage Collection\n 4. Subtle v8 ES6 perfomance issues\n\nThe Base Line\nTo begin, there are already a few things beyond  setting the mode in \nwebpack.config.js  we already  apply so we’re not doing too much optimization\nduring incremental builds:\n\noptimization: {\n  removeAvailableModules: false,\n  removeEmptyChunks: false,\n  splitChunks: false,\n}\n\n\nAlright, so let’s establish the baseline by looking at a typical inner loop\nflame graph:\n\nAs you can see, we’re clocking in at around 40s here per incremental build. This\nis not exactly true because we lose about 5s of it due to profiling. In\nmeasuring with our internal telemetry, we noticed that our devs are hitting\naround 30–35s on avg (and sometimes over a minute at the 75th percentile)\nincremental builds.\n\nSo, as soon as you look at those colors, you would recognize three separate\nphases of the incremental build process. With this in mind, let’s tackle the\nfirst enemy.\n\nEnemy #1: stats.toJson is VERY heavy in WP4\nIf you were looking at the the CPU profile flame graph, you would notice that\nthe last phase of the process is dominated by a bunch of stats.toJson  calls.\nWhere does it come from? It’s right inside webpack-dev-server’s Server.js:\n\nconst clientStats = { errorDetails: false };\n...\ncomp.hooks.done.tap('webpack-dev-server', (stats) => {\n  this._sendStats(this.sockets, stats.toJson(clientStats));\n  this._stats = stats;\n});\n\n\nThe issue here is that Webpack 4 gave toJson a lot more information, but it also\nregressed the performance tremendously as a result. The fix is in a pull\nrequest:\n\nhttps://github.com/webpack/webpack-dev-server/pull/1362\n\nThis is the big one — it brought our incremental speeds from 30s to around 15s.\n\nUpdate: the webpack-dev-server maintainers had accepted my patch! So, go ahead\nand use webpack-dev-server@3.1.2. Personally, I have observed a slight 0.5s\nregression between 2.x release and the 3.x release, so we’re keeping the 2.x for\nnow until we can move to using webpack-serve.\n\nSince we’re waiting for the authors to merge this for v2, I’ve published a\ntemporary fork for it for the v2 branch:\n\nhttps://www.npmjs.com/package/webpack-dev-server-speedy\n\nFor the fix on v2, you’ll have to use the node API to take advantage of it like\nthis package in your build process:\n\nconst Webpack = require('webpack');\nconst WebpackDevServer = require('webpack-dev-server-speedy');\nconst webpackConfig = require('./webpack.config');\nconst compiler = Webpack(webpackConfig);\nconst devServerOptions = Object.assign({}, webpackConfig.devServer, {\n  stats: {\n    colors: true\n  }\n});\nconst server = new WebpackDevServer(compiler, devServerOptions);\nserver.listen(8080, '127.0.0.1', () => {\n  console.log('Starting server on http://localhost:8080');\n});\n\n\nEnemy #2: Competing resolution logic between Webpack and ts-loader\nIf I were to ask you to build an incremental compiler based on Typescript, you\nwould likely first reach into the Typescript API for something that it is using\nfor its watch mode. For the longest time, Typescript safe guarded this API from\nexternal modules. During this time, ts-loader was born. The author of the loader\ntracked the progress of another Typescript-centric loader called\nawesome-typescript-loader and brought back the idea of doing type checking on a\nseparate thread. This transpileOnly  flag worked remarkably well (with a rather\nglaring caveat that const enums are not supported out of the box — here’s a \nworkaround [https://github.com/kulshekhar/ts-jest#const-enum-is-not-supported] \nfrom the ts-jest repo) until the codebase reaches a certain size.\n\nIn OWA, we have nearly 9000 modules that we shove across this loader. We have\nfound that the first phase of that incremental build is linearly growing as our\nrepo grows.\n\nThings looked pretty grim until the Typescript team decided to take on this\nmammoth work of expose the watch API to external modules. Specifically, after \nthis [https://github.com/TypeStrong/ts-loader/pull/685]  was merged, ts-loader\nis super charged with the ability to limit the amount of modules to transpile at\na time per iteration!\n\nWe just add this to our webpack config module.rules:\n\n{\n  test: /\\.tsx?$/,\n  use: [\n    {\n      loader: 'ts-loader',\n      options: {\n        transpileOnly: true,\n        experimentalWatchApi: true,\n      },\n    },\n  ],\n}\n\n\nDon’t forget to the typechecker when appropriate: \nhttps://www.npmjs.com/package/fork-ts-checker-webpack-plugin  (we have a mode to\nturn type checker OFF for even faster rebuilds)\n\nThe incremental builds now only rebuilds around 30–40 modules rather than 50% of\nour modules! I also have a way to CAP the growth of the incremental builds in\nthe first phase.\n\nThis optimization cuts our 15s to around 8s.\n\nEnemy #3: Garbage Collection\nSo Garbage Collection is a great invention. But not in a tight loop. Perhaps\nthere’s a perf bug inside node or v8, but I’ve discovered that a global \nstring.replace(/…/g, ‘…..’) can cause a lot of GC  when placed inside a loop.\nWebpack 4 introduced the path info in the generated dev mode replacing module\nids with more useful path info. This is done with, you guessed it, global string\nreplace with regex. It then created a LOT of unnecessary GCs along the way. (as\nan aside, perhaps I should file a bug against either Webpack, node, or v8…)\n\nOkay, let’s turn that sucker off in webpack.config.js in the output.pathinfo:\n\noutput: {\n  pathinfo: false\n}\n\nJust ask yourself if you REALLY need that pathinfo or that build speed. For us,\nwe chose speed. This made our 8s builds to around 6s\n\nEnemy #4: Subtle v8 ES6 perfomance issues\nMost everyone would be pleased with that 6s figure, but why should we humans not\ndemand MOAR? Yes, MOAR speed!!!\n\nIn chatting with a colleague of mine, John-David Dalton\n[https://github.com/jdalton], about his project, esm\n[https://medium.com/web-on-the-edge/tomorrows-es-modules-today-c53d29ac448c], he\ntold me about node.js performance issues with ES6 data structures like Map and\nSet. Having dug into Webpack source code previously and  by looking at the\nremaining profile slowdowns (looking at the “heavy” or “bottom-up”), I noticed\nthat Webpack’s internal algorithm is dominated by calling their SortableSet\nmethods. Since SortableSet extends Set, it would follow that Webpack is actually\ngreatly affected by the speed of the Map/Set implementation of V8. Here’s the\nbug:\n\nhttps://github.com/nodejs/node/issues/19769\n\nSo, I advise everyone doing heavy Webpack development to switch to the LTS (v10+\nor stick with v8.9.4)\n\nUsing that version, the incremental build is down to 4.5s\n\nWhy? Inventing on Principle!\nFinally, I want to leave you with the best motivation on why we should reduce\nthis incremental build speeds down to almost nothing:\n\nHey! follow me on twitter @kenneth_chau [https://twitter.com/kenneth_chau]  to\nget more articles like these :)","html":"<p>What if I told you your Webpack is doing too much work all this time? Webpack 4 brought a lot of goodies for developers but to use it at scale, the <a href=\"http://outlook.com/\" rel=\"nofollow noopener noopener\">Outlook</a> team at Microsoft had to take a hard look at the incremental build numbers to find out. Here’s how we made our incremental builds go from 35s to a consistent 5s.</p><p><em>I guess it goes without saying that you MUST measure in order for you to know you have made progress!</em></p><p>Let’s name some enemies of incremental build speed:</p><ol><li><a href=\"https://webpack.js.org/api/node/#stats-tojson-options-\" rel=\"noopener nofollow\">stats.toJson</a>()</li><li>Competing resolution logic between Webpack and its loaders (<a href=\"https://github.com/TypeStrong/ts-loader\" rel=\"noopener nofollow\">ts-loader</a>)</li><li>Garbage Collection</li><li>Subtle v8 ES6 perfomance issues</li></ol><h3 id=\"the-base-line\">The Base Line</h3><p>To begin, there are already a few things <em>beyond</em> setting the mode in <code>webpack.config.js</code> we <em>already</em> apply so we’re not doing too much optimization during incremental builds:</p><!--kg-card-begin: markdown--><pre><code class=\"language-js\">optimization: {\n  removeAvailableModules: false,\n  removeEmptyChunks: false,\n  splitChunks: false,\n}\n</code></pre>\n<!--kg-card-end: markdown--><p>Alright, so let’s establish the baseline by looking at a typical inner loop flame graph:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card\"><img src=\"http://localhost:2368/content/images/2019/03/image.png\" class=\"kg-image\"></figure><!--kg-card-end: image--><p>As you can see, we’re clocking in at around 40s here per incremental build. This is not exactly true because we lose about 5s of it due to profiling. In measuring with our internal telemetry, we noticed that our devs are hitting around 30–35s on avg (and sometimes over a minute at the 75th percentile) incremental builds.</p><p>So, as soon as you look at those colors, you would recognize three separate phases of the incremental build process. With this in mind, let’s tackle the first enemy.</p><h3 id=\"enemy-1-stats-tojson-is-very-heavy-in-wp4\">Enemy #1: stats.toJson is VERY heavy in WP4</h3><p>If you were looking at the the CPU profile flame graph, you would notice that the last phase of the process is dominated by a bunch of <strong><strong>stats.toJson</strong></strong> calls. Where does it come from? It’s right inside <strong><strong>webpack-dev-server</strong></strong>’s Server.js:</p><!--kg-card-begin: markdown--><pre><code class=\"language-js\">const clientStats = { errorDetails: false };\n...\ncomp.hooks.done.tap('webpack-dev-server', (stats) =&gt; {\n  this._sendStats(this.sockets, stats.toJson(clientStats));\n  this._stats = stats;\n});\n</code></pre>\n<!--kg-card-end: markdown--><p>The issue here is that Webpack 4 gave toJson a lot more information, but it also regressed the performance tremendously as a result. The fix is in a pull request:</p><p><a href=\"https://github.com/webpack/webpack-dev-server/pull/1362\">https://github.com/webpack/webpack-dev-server/pull/1362</a></p><p><strong><strong>This is the big one — it brought our incremental speeds from 30s to around 15s.</strong></strong></p><p><strong><strong>Update</strong></strong>: the webpack-dev-server maintainers had accepted my patch! So, go ahead and use webpack-dev-server@3.1.2. Personally, I have observed a slight 0.5s regression between 2.x release and the 3.x release, so we’re keeping the 2.x for now until we can move to using webpack-serve.</p><p>Since we’re waiting for the authors to merge this for v2, I’ve published a temporary fork for it for the v2 branch:</p><p><a href=\"https://www.npmjs.com/package/webpack-dev-server-speedy\">https://www.npmjs.com/package/webpack-dev-server-speedy</a></p><p>For the fix on v2, you’ll have to use the node API to take advantage of it like this package in your build process:</p><!--kg-card-begin: markdown--><pre><code class=\"language-js\">const Webpack = require('webpack');\nconst WebpackDevServer = require('webpack-dev-server-speedy');\nconst webpackConfig = require('./webpack.config');\nconst compiler = Webpack(webpackConfig);\nconst devServerOptions = Object.assign({}, webpackConfig.devServer, {\n  stats: {\n    colors: true\n  }\n});\nconst server = new WebpackDevServer(compiler, devServerOptions);\nserver.listen(8080, '127.0.0.1', () =&gt; {\n  console.log('Starting server on http://localhost:8080');\n});\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"enemy-2-competing-resolution-logic-between-webpack-and-ts-loader\">Enemy #2: Competing resolution logic between Webpack and ts-loader</h3><p>If I were to ask you to build an incremental compiler based on Typescript, you would likely first reach into the Typescript API for something that it is using for its watch mode. For the longest time, Typescript safe guarded this API from external modules. During this time, ts-loader was born. The author of the loader tracked the progress of another Typescript-centric loader called awesome-typescript-loader and brought back the idea of doing type checking on a separate thread. This <em>transpileOnly</em> flag worked remarkably well (with a rather glaring caveat that const enums are not supported out of the box — here’s a <a href=\"https://github.com/kulshekhar/ts-jest#const-enum-is-not-supported\" rel=\"noopener nofollow\">workaround</a> from the ts-jest repo) until the codebase reaches a certain size.</p><p>In OWA, we have nearly 9000 modules that we shove across this loader. We have found that the first phase of that incremental build is linearly growing as our repo grows.</p><p>Things looked pretty grim until the Typescript team decided to take on this mammoth work of expose the watch API to external modules. Specifically, after <a href=\"https://github.com/TypeStrong/ts-loader/pull/685\" rel=\"noopener nofollow\">this</a> was merged, ts-loader is super charged with the ability to limit the amount of modules to transpile at a time per iteration!</p><p>We just add this to our webpack config <strong><strong>module.rules</strong></strong>:</p><!--kg-card-begin: markdown--><pre><code>{\n  test: /\\.tsx?$/,\n  use: [\n    {\n      loader: 'ts-loader',\n      options: {\n        transpileOnly: true,\n        experimentalWatchApi: true,\n      },\n    },\n  ],\n}\n</code></pre>\n<!--kg-card-end: markdown--><p><em>Don’t forget to the typechecker when appropriate: </em><a href=\"https://www.npmjs.com/package/fork-ts-checker-webpack-plugin\" rel=\"nofollow noopener nofollow noopener\"><em>https://www.npmjs.com/package/fork-ts-checker-webpack-plugin</em></a><em> (we have a mode to turn type checker OFF for even faster rebuilds)</em></p><p>The incremental builds now only rebuilds around 30–40 modules rather than 50% of our modules! I also have a way to CAP the growth of the incremental builds in the first phase.</p><p><strong><strong>This optimization cuts our 15s to around 8s</strong></strong>.</p><h3 id=\"enemy-3-garbage-collection\"><strong>Enemy #3: Garbage Collection</strong></h3><p>So Garbage Collection is a great invention. But not in a tight loop. Perhaps there’s a perf bug inside node or v8, but I’ve discovered that a global <strong><strong>string.replace(/…/g, ‘…..’) can cause a lot of GC</strong></strong> when placed inside a loop. Webpack 4 introduced the path info in the generated dev mode replacing module ids with more useful path info. This is done with, you guessed it, global string replace with regex. It then created a LOT of unnecessary GCs along the way. (<em>as an aside, perhaps I should file a bug against either Webpack, node, or v8…</em>)</p><p>Okay, let’s turn that sucker off in webpack.config.js in the output.pathinfo:</p><!--kg-card-begin: code--><pre><code>output: {\n  pathinfo: false\n}</code></pre><!--kg-card-end: code--><p>Just ask yourself if you REALLY need that pathinfo or that build speed. For us, we chose speed. <strong><strong>This made our 8s builds to around 6s</strong></strong></p><h3 id=\"enemy-4-subtle-v8-es6-perfomance-issues\">Enemy #4: Subtle v8 ES6 perfomance issues</h3><p>Most everyone would be pleased with that 6s figure, but why should we humans not demand MOAR? Yes, MOAR speed!!!</p><p>In chatting with a colleague of mine, <a href=\"https://github.com/jdalton\" rel=\"nofollow noopener\">John-David Dalton</a>, about his project, <a href=\"https://medium.com/web-on-the-edge/tomorrows-es-modules-today-c53d29ac448c\">esm</a>, he told me about node.js performance issues with ES6 data structures like Map and Set. Having dug into Webpack source code previously <em>and</em> by looking at the remaining profile slowdowns (looking at the “heavy” or “bottom-up”), I noticed that Webpack’s internal algorithm is dominated by calling their SortableSet methods. Since SortableSet extends Set, it would follow that Webpack is actually greatly affected by the speed of the Map/Set implementation of V8. Here’s the bug:</p><p><a href=\"https://github.com/nodejs/node/issues/19769\">https://github.com/nodejs/node/issues/19769</a></p><p>So, I advise everyone doing heavy Webpack development to switch to the LTS (v10+ or stick with v8.9.4)</p><p><strong><strong>Using that version, the incremental build is down to 4.5s</strong></strong></p><h3 id=\"why-inventing-on-principle-\">Why? Inventing on Principle!</h3><p>Finally, I want to leave you with the best motivation on why we should reduce this incremental build speeds down to almost nothing:</p><!--kg-card-begin: embed--><figure class=\"kg-card kg-embed-card\"><iframe src=\"https://player.vimeo.com/video/36579366?app_id=122963\" width=\"480\" height=\"270\" frameborder=\"0\" title=\"Bret Victor - Inventing on Principle\" allow=\"autoplay; fullscreen\" allowfullscreen></iframe></figure><!--kg-card-end: embed--><p>Hey! follow me on twitter <a href=\"https://twitter.com/kenneth_chau\">@kenneth_chau</a> to get more articles like these :)</p>","url":"http://localhost:2368/speeding-up-webpack-typescript-incremental-builds-by-7x/","uuid":"6d4ea64f-f9fa-4ce9-9133-7f5963a8a862","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c9aa8bfb6d76300017e5840"}},"pageContext":{"slug":"speeding-up-webpack-typescript-incremental-builds-by-7x"}}