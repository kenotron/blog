<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Crash Test Dev]]></title><description><![CDATA[Blog of a curious developer]]></description><link>crashtestdev.com/</link><image><url>crashtestdev.com/favicon.png</url><title>Crash Test Dev</title><link>crashtestdev.com/</link></image><generator>Ghost 2.9</generator><lastBuildDate>Thu, 28 Mar 2019 05:35:05 GMT</lastBuildDate><atom:link href="crashtestdev.com/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Localize React without Bloating the Bundle]]></title><description><![CDATA[There are so many possibilities when you want to localize your application with
your React application. I believe that localization is difficult because it
requires excellence in several pieces of a stack at the same time. You have to
have a working pipeline that can take string resources that would get
translated. Then, you have to have a way to load these strings onto your page or
application. Finally, you have to have a way to take these strings and inject
them into the components inside your]]></description><link>http://localhost:2368/localize-react-apps-the-performant-way/</link><guid isPermaLink="false">Ghost__Post__5c9a41371046a4000156bcf3</guid><dc:creator><![CDATA[Ken Chau]]></dc:creator><pubDate>Tue, 26 Mar 2019 17:55:16 GMT</pubDate><media:content url="http://localhost:2368/content/images/2019/03/roman-kraft-136249-unsplash-1.jpg" medium="image"/><content:encoded><![CDATA[<img src="http://localhost:2368/content/images/2019/03/roman-kraft-136249-unsplash-1.jpg" alt="Localize React without Bloating the Bundle"/><p>There are so many possibilities when you want to localize your application with your React application. I believe that localization is difficult because it requires excellence in several pieces of a stack at the same time. You have to have a working pipeline that can take string resources that would get translated. Then, you have to have a way to load these strings onto your page or application. Finally, you have to have a way to take these strings and inject them into the components inside your application. As you can see, you can choose just about any tech to help you accomplish these goals. I am documenting a particular set of stack that I believe helps you achieve this in the most performant way with tools that you're already probably using.</p><h2 id="translation-pipeline">Translation Pipeline</h2><p>At <a href="https://github.com/officedev/office-ui-fabric-react">work</a>, we have a translation-as-a-service API that we rely on to refresh localized strings for us every night. There's a growing team that has built a simple <a href="https://devops.azure.com">Azure DevOps</a> task we add as a step in one of our pipelines which runs nightly. </p><p>Not everyone is as fortunate that has something they can use from their own company. Given that, I'll suggest a pattern here as the first step. Do a search for "localization as a service" and look for a vendor that can help add a step in your CI pipeline of choice. Set up a nightly job to refresh your application's localized strings as JSON like this:</p><!--kg-card-begin: markdown--><pre><code class="language-json">{
  &quot;HELLO_NAME&quot;: &quot;Hello {name}!&quot;,
  &quot;CLICK_ME&quot;: &quot;Click me&quot;
}
</code></pre>
<!--kg-card-end: markdown--><h2 id="rendering-localized-strings">Rendering Localized Strings</h2><p>React is a large ecosystem. So then the paradox of choice is real when finding supplemental libraries for React. Conventional wisdom is to find the most popular packages from npmjs.org. So given this, I first looked at <code>react-intl</code> to help me inject localized strings into my application. The issue here is that <code>react-intl</code> uses higher order components all over the place. One of the explicit goals (as I heard it from ReactConf 2018) of React hooks is to do away with depth of the component tree caused by the higher order components. Higher order component, or HOC, is a neat idea until the consumer needed to access the ref to the original wrapped component. When all your components that use localized strings are wrapped in HOCs, your application start to look like a sideways mountain. (aside: go look at your component tree in React DevTool to see if you're suffering from HOC-itis)</p><p>Enter <code><a href="https://github.com/alibaba/react-intl-universal">react-intl-universal</a></code>. The Alibaba Group created this library to get around the HOC issues of <code>react-intl</code>. On top of this, there are times where strings are needed from outside of the component's <code>render()</code> method. It takes 2 steps to place strings inside your components. </p><p>First you have to initialize the locale data. Note that the data can be preloaded from a server or can be retrieved at runtime. The choice is yours. For the most optimal case, we definitely would have the server preload strings right in the app as it is being loaded.</p><p>Let's pretend that <code>./locales/en-US.json</code> has the same content as the example above.</p><!--kg-card-begin: markdown--><pre><code class="language-jsx">// locale data
const locales = {
  &quot;en-US&quot;: require('./locales/en-US.json'),
  &quot;zh-CN&quot;: require('./locales/zh-CN.json'),
};
</code></pre>
<!--kg-card-end: markdown--><p>Then, we initialize the <code>react-intl-universal</code> library inside a <code>componentDidMount()</code> call. And we'll use the localized string inside the <code>render()</code> method with the <code>.get()</code> function: </p><!--kg-card-begin: markdown--><pre><code class="language-jsx">import intl from 'react-intl-universal'; 

class App extends Component {
  state = { isLoading: false }
  componentDidMount() {
    this.loadLocales();
  }

  async loadLocales() {
    await intl.init({
      currentLocale: 'en-US',
      locales,
    });
    this.setState({ isLoading: true });
  }

  render() {
    return (
      !this.state.isLoading &amp;&amp;
      &lt;div&gt;
        {intl.get('HELLO_NAME', {name: 'world'})}
      &lt;/div&gt;
    );
  }

}
</code></pre>
<!--kg-card-end: markdown--><p>Note that the <code>init()</code> call returns a <code>Promise</code>. This means that we can use the async / await syntax to write our string load code. Once this is added, we look at the way we retrieve the strings by key. For that, we use the <code>get()</code>. Get takes in two parameters: the key and some object. Sometimes the strings have slots that can be replaced by the object values.</p><h3 id="loading-localized-strings">Loading Localized Strings</h3><p>This is where it gets interesting. So far, we've assumed that we had the locale data all upfront. This means that all the localized strings would had been loaded inside a bundle or onto the page somehow. Loading all the language strings in one go can only be feasible if the app barely contain any text. If we're using Webpack, we should take advantage of a feature that I recently came to know. We all have seen the dynamic <code>import()</code> syntax:</p><!--kg-card-begin: markdown--><pre><code class="language-js">const SomeModule = import('some-module');
</code></pre>
<!--kg-card-end: markdown--><p>But, have you seen what Webpack can do with something like this?</p><!--kg-card-begin: markdown--><pre><code class="language-js">const getLocale = (locale) =&gt; import(`./locales/${locale}.json`);
</code></pre>
<!--kg-card-end: markdown--><p>Based on the <code>.json</code> files it finds inside <code>./locales</code>, Webpack is smart enough to generate chunks for dynamic loading! That means your main bundle will not incur the weight of the entire library of localized strings. Putting all these concepts together, I've created a repo to demonstrate concepts from this post:</p><p><a href="https://github.com/kenotron/react-intl-example">https://github.com/kenotron/react-intl-example</a></p><p>I'll go over some of the points from that repo. First, I created a HOC that you place at the ROOT of the application. Don't worry! It is only one HOC for the entire app. It is called <code>LocaleComponent</code> - I'm keeping this strange little name until <code>React.createResource()</code> becomes a thing maybe in the future.</p><!--kg-card-begin: markdown--><pre><code class="language-jsx">const getLocale = locale =&gt; import(`./locale/${locale}.json`);

class LocaleComponent extends React.Component {
  state = { isLoading: true };
  
  async loadLocales() { 
    const locales = await getLocale('en');
    const currentLocale = 'en';
    await intl.init({ currentLocale, locales });
    this.setState({ isLoading: false });
  }
  
  render() {
    return !this.state.isLoading ? &lt;&gt;this.props.children&lt;/&gt; : null;
  }
}
</code></pre>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Speeding Up Webpack, Typescript Incremental Builds by 7x]]></title><description><![CDATA[What if I told you your Webpack is doing too much work all this time? Webpack 4
brought a lot of goodies for developers but to use it at scale, the Outlook
[http://outlook.com/]  team at Microsoft had to take a hard look at the
incremental build numbers to find out. Here’s how we made our incremental builds
go from 35s to a consistent 5s.

I guess it goes without saying that you MUST measure in order for you to know
you have made progress!

Let’s name some enemies of incremental build speed:

 1]]></description><link>http://localhost:2368/speeding-up-webpack-typescript-incremental-builds-by-7x/</link><guid isPermaLink="false">Ghost__Post__5c9aa8bfb6d76300017e5840</guid><dc:creator><![CDATA[Ken Chau]]></dc:creator><pubDate>Fri, 06 Apr 2018 22:58:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1534078362425-387ae9668c17?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=1080&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1534078362425-387ae9668c17?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=1080&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ" alt="Speeding Up Webpack, Typescript Incremental Builds by 7x"/><p>What if I told you your Webpack is doing too much work all this time? Webpack 4 brought a lot of goodies for developers but to use it at scale, the <a href="http://outlook.com/" rel="nofollow noopener noopener">Outlook</a> team at Microsoft had to take a hard look at the incremental build numbers to find out. Here’s how we made our incremental builds go from 35s to a consistent 5s.</p><p><em>I guess it goes without saying that you MUST measure in order for you to know you have made progress!</em></p><p>Let’s name some enemies of incremental build speed:</p><ol><li><a href="https://webpack.js.org/api/node/#stats-tojson-options-" rel="noopener nofollow">stats.toJson</a>()</li><li>Competing resolution logic between Webpack and its loaders (<a href="https://github.com/TypeStrong/ts-loader" rel="noopener nofollow">ts-loader</a>)</li><li>Garbage Collection</li><li>Subtle v8 ES6 perfomance issues</li></ol><h3 id="the-base-line">The Base Line</h3><p>To begin, there are already a few things <em>beyond</em> setting the mode in <code>webpack.config.js</code> we <em>already</em> apply so we’re not doing too much optimization during incremental builds:</p><!--kg-card-begin: markdown--><pre><code class="language-js">optimization: {
  removeAvailableModules: false,
  removeEmptyChunks: false,
  splitChunks: false,
}
</code></pre>
<!--kg-card-end: markdown--><p>Alright, so let’s establish the baseline by looking at a typical inner loop flame graph:</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2019/03/image.png" class="kg-image" alt="Speeding Up Webpack, Typescript Incremental Builds by 7x"/></figure><!--kg-card-end: image--><p>As you can see, we’re clocking in at around 40s here per incremental build. This is not exactly true because we lose about 5s of it due to profiling. In measuring with our internal telemetry, we noticed that our devs are hitting around 30–35s on avg (and sometimes over a minute at the 75th percentile) incremental builds.</p><p>So, as soon as you look at those colors, you would recognize three separate phases of the incremental build process. With this in mind, let’s tackle the first enemy.</p><h3 id="enemy-1-stats-tojson-is-very-heavy-in-wp4">Enemy #1: stats.toJson is VERY heavy in WP4</h3><p>If you were looking at the the CPU profile flame graph, you would notice that the last phase of the process is dominated by a bunch of <strong><strong>stats.toJson</strong></strong> calls. Where does it come from? It’s right inside <strong><strong>webpack-dev-server</strong></strong>’s Server.js:</p><!--kg-card-begin: markdown--><pre><code class="language-js">const clientStats = { errorDetails: false };
...
comp.hooks.done.tap('webpack-dev-server', (stats) =&gt; {
  this._sendStats(this.sockets, stats.toJson(clientStats));
  this._stats = stats;
});
</code></pre>
<!--kg-card-end: markdown--><p>The issue here is that Webpack 4 gave toJson a lot more information, but it also regressed the performance tremendously as a result. The fix is in a pull request:</p><p><a href="https://github.com/webpack/webpack-dev-server/pull/1362">https://github.com/webpack/webpack-dev-server/pull/1362</a></p><p><strong><strong>This is the big one — it brought our incremental speeds from 30s to around 15s.</strong></strong></p><p><strong><strong>Update</strong></strong>: the webpack-dev-server maintainers had accepted my patch! So, go ahead and use webpack-dev-server@3.1.2. Personally, I have observed a slight 0.5s regression between 2.x release and the 3.x release, so we’re keeping the 2.x for now until we can move to using webpack-serve.</p><p>Since we’re waiting for the authors to merge this for v2, I’ve published a temporary fork for it for the v2 branch:</p><p><a href="https://www.npmjs.com/package/webpack-dev-server-speedy">https://www.npmjs.com/package/webpack-dev-server-speedy</a></p><p>For the fix on v2, you’ll have to use the node API to take advantage of it like this package in your build process:</p><!--kg-card-begin: markdown--><pre><code class="language-js">const Webpack = require('webpack');
const WebpackDevServer = require('webpack-dev-server-speedy');
const webpackConfig = require('./webpack.config');
const compiler = Webpack(webpackConfig);
const devServerOptions = Object.assign({}, webpackConfig.devServer, {
  stats: {
    colors: true
  }
});
const server = new WebpackDevServer(compiler, devServerOptions);
server.listen(8080, '127.0.0.1', () =&gt; {
  console.log('Starting server on http://localhost:8080');
});
</code></pre>
<!--kg-card-end: markdown--><h3 id="enemy-2-competing-resolution-logic-between-webpack-and-ts-loader">Enemy #2: Competing resolution logic between Webpack and ts-loader</h3><p>If I were to ask you to build an incremental compiler based on Typescript, you would likely first reach into the Typescript API for something that it is using for its watch mode. For the longest time, Typescript safe guarded this API from external modules. During this time, ts-loader was born. The author of the loader tracked the progress of another Typescript-centric loader called awesome-typescript-loader and brought back the idea of doing type checking on a separate thread. This <em>transpileOnly</em> flag worked remarkably well (with a rather glaring caveat that const enums are not supported out of the box — here’s a <a href="https://github.com/kulshekhar/ts-jest#const-enum-is-not-supported" rel="noopener nofollow">workaround</a> from the ts-jest repo) until the codebase reaches a certain size.</p><p>In OWA, we have nearly 9000 modules that we shove across this loader. We have found that the first phase of that incremental build is linearly growing as our repo grows.</p><p>Things looked pretty grim until the Typescript team decided to take on this mammoth work of expose the watch API to external modules. Specifically, after <a href="https://github.com/TypeStrong/ts-loader/pull/685" rel="noopener nofollow">this</a> was merged, ts-loader is super charged with the ability to limit the amount of modules to transpile at a time per iteration!</p><p>We just add this to our webpack config <strong><strong>module.rules</strong></strong>:</p><!--kg-card-begin: markdown--><pre><code>{
  test: /\.tsx?$/,
  use: [
    {
      loader: 'ts-loader',
      options: {
        transpileOnly: true,
        experimentalWatchApi: true,
      },
    },
  ],
}
</code></pre>
<!--kg-card-end: markdown--><p><em>Don’t forget to the typechecker when appropriate: </em><a href="https://www.npmjs.com/package/fork-ts-checker-webpack-plugin" rel="nofollow noopener nofollow noopener"><em>https://www.npmjs.com/package/fork-ts-checker-webpack-plugin</em></a><em> (we have a mode to turn type checker OFF for even faster rebuilds)</em></p><p>The incremental builds now only rebuilds around 30–40 modules rather than 50% of our modules! I also have a way to CAP the growth of the incremental builds in the first phase.</p><p><strong><strong>This optimization cuts our 15s to around 8s</strong></strong>.</p><h3 id="enemy-3-garbage-collection"><strong>Enemy #3: Garbage Collection</strong></h3><p>So Garbage Collection is a great invention. But not in a tight loop. Perhaps there’s a perf bug inside node or v8, but I’ve discovered that a global <strong><strong>string.replace(/…/g, ‘…..’) can cause a lot of GC</strong></strong> when placed inside a loop. Webpack 4 introduced the path info in the generated dev mode replacing module ids with more useful path info. This is done with, you guessed it, global string replace with regex. It then created a LOT of unnecessary GCs along the way. (<em>as an aside, perhaps I should file a bug against either Webpack, node, or v8…</em>)</p><p>Okay, let’s turn that sucker off in webpack.config.js in the output.pathinfo:</p><!--kg-card-begin: code--><pre><code>output: {
  pathinfo: false
}</code></pre><!--kg-card-end: code--><p>Just ask yourself if you REALLY need that pathinfo or that build speed. For us, we chose speed. <strong><strong>This made our 8s builds to around 6s</strong></strong></p><h3 id="enemy-4-subtle-v8-es6-perfomance-issues">Enemy #4: Subtle v8 ES6 perfomance issues</h3><p>Most everyone would be pleased with that 6s figure, but why should we humans not demand MOAR? Yes, MOAR speed!!!</p><p>In chatting with a colleague of mine, <a href="https://github.com/jdalton" rel="nofollow noopener">John-David Dalton</a>, about his project, <a href="https://medium.com/web-on-the-edge/tomorrows-es-modules-today-c53d29ac448c">esm</a>, he told me about node.js performance issues with ES6 data structures like Map and Set. Having dug into Webpack source code previously <em>and</em> by looking at the remaining profile slowdowns (looking at the “heavy” or “bottom-up”), I noticed that Webpack’s internal algorithm is dominated by calling their SortableSet methods. Since SortableSet extends Set, it would follow that Webpack is actually greatly affected by the speed of the Map/Set implementation of V8. Here’s the bug:</p><p><a href="https://github.com/nodejs/node/issues/19769">https://github.com/nodejs/node/issues/19769</a></p><p>So, I advise everyone doing heavy Webpack development to switch to the LTS (v10+ or stick with v8.9.4)</p><p><strong><strong>Using that version, the incremental build is down to 4.5s</strong></strong></p><h3 id="why-inventing-on-principle-">Why? Inventing on Principle!</h3><p>Finally, I want to leave you with the best motivation on why we should reduce this incremental build speeds down to almost nothing:</p><!--kg-card-begin: embed--><figure class="kg-card kg-embed-card"><iframe src="https://player.vimeo.com/video/36579366?app_id=122963" width="480" height="270" frameborder="0" title="Bret Victor - Inventing on Principle" allow="autoplay; fullscreen" allowfullscreen=""/></figure><!--kg-card-end: embed--><p>Hey! follow me on twitter <a href="https://twitter.com/kenneth_chau">@kenneth_chau</a> to get more articles like these :)</p>]]></content:encoded></item></channel></rss>